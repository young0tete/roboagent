{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df20297",
   "metadata": {},
   "source": [
    "# Team ALOHA Code\n",
    "0. 라이브러리 임포트\n",
    "1. 데이터 불러오기\n",
    "2. 필요 함수 정의\n",
    "3. 데이터 전처리\n",
    "4. 모델링\n",
    "5. 참조\n",
    "6. 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee3af2",
   "metadata": {},
   "source": [
    "# 0. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import FinanceDataReader as fdr\n",
    "import pandas_datareader as pdr\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import plotly.express as px\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pytimekr import pytimekr\n",
    "import datetime\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "import warnings                 # warning 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ssl    # Mac에서 FinanceDataReader가 안나오는 오류 해결\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from matplotlib import rc       # 한글 깨짐 방지\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f858886",
   "metadata": {},
   "source": [
    "# 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327cf6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Jan=pd.read_csv(\"NASDAQ_RSS_IFO_202301.csv\", encoding='CP949')\n",
    "Feb=pd.read_csv(\"NASDAQ_RSS_IFO_202302.csv\", encoding='CP949')\n",
    "Mar=pd.read_csv(\"NASDAQ_RSS_IFO_202303.csv\", encoding='CP949')\n",
    "Apr=pd.read_csv(\"NASDAQ_RSS_IFO_202304.csv\", encoding='CP949')\n",
    "May=pd.read_csv(\"NASDAQ_RSS_IFO_202305.csv\", encoding='CP949')\n",
    "Jun=pd.read_csv(\"NASDAQ_RSS_IFO_202306.csv\", encoding='CP949')\n",
    "Jul=pd.read_csv(\"NASDAQ_RSS_IFO_202307.csv\", encoding='CP949')\n",
    "Aug=pd.read_csv(\"NASDAQ_RSS_IFO_202308.csv\", encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a4b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([Jan, Feb, Mar, Apr, May, Jun, Jul, Aug], axis=0).reset_index(drop=True)\n",
    "df=df.drop_duplicates(subset=['rgs_dt', 'tck_iem_cd'], keep='first').reset_index(drop=True).dropna()\n",
    "df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cef8a",
   "metadata": {},
   "source": [
    "# 2. 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ee323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 변환 함수 (days)\n",
    "# Input // date: 기준 날짜(YYYY-MM-DD, dtype:int), days: 변경되는 days(dtype:int)\n",
    "# Output // YYYY-MM-DD (dtype:str)\n",
    "\n",
    "def time_delta_days(date, days):                                           # 시간 변환 함수\n",
    "    if len(str(date))==8:\n",
    "        dtend=datetime.datetime.strptime(str(date), '%Y%m%d')              # string -> datetime\n",
    "    else:\n",
    "        dtend=date\n",
    "    result=dtend+datetime.timedelta(days=days)    \n",
    "    return result                                                          # datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb987a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input // df: dataframe iloc: df의 iloc(n, dtype:int), days1: 원하는 범위1(n, dtype:int), days1: 원하는 범위2(n, dtype:int)\n",
    "# Output // 사건 발생 시점의 -days1 ~ +days2까지의 주식 데이터의 dataframe\n",
    "def ReadingData(df, iloc, days1, days2):\n",
    "    start=time_delta_days(df.iloc[iloc]['rgs_dt'],days1)\n",
    "    end=time_delta_days(df.iloc[iloc]['rgs_dt'],days2)\n",
    "    # print(df.iloc[iloc]['tck_iem_cd'], start+' ~ '+end)\n",
    "    return pd.DataFrame(fdr.DataReader(df.iloc[iloc]['tck_iem_cd'], start=start, end=end).Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c64ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input // df: f주식 데이터의 dataframe(dtype: DataFrame), date: 원하는 MovingAverage(n,dtype:int)\n",
    "# Output // [가격, ~일 이동평균선]의 약 1년 DataFrame\n",
    "\n",
    "def MA(df, date):\n",
    "    rolling_mean=df['Close'].rolling(window = date).mean()\n",
    "    rolling_mean.name = f'{date} MA'\n",
    "    curve=pd.concat([df, rolling_mean], axis=1)\n",
    "    return curve[-1:-250:-1].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc621df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input // \n",
    "\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=df['rgs_dt'].min(), end=df['rgs_dt'].max())\n",
    "weekdays = pd.date_range(start=df['rgs_dt'].min(), end=df['rgs_dt'].max(), freq='B')\n",
    "\n",
    "def replace_with_nearest_future_weekday(date):\n",
    "    if date in holidays or date not in weekdays:\n",
    "        next_weekday = date + pd.DateOffset(days=1)\n",
    "        while next_weekday not in weekdays:\n",
    "            next_weekday += pd.DateOffset(days=1)\n",
    "        return next_weekday.date()\n",
    "    else:\n",
    "        return date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b21e8",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ee5f1",
   "metadata": {},
   "source": [
    "## 3-1) 주말 및 공휴일 날짜를 평일 날짜로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'rgs_dt' 열의 날짜를 대체\n",
    "df['rgs_dt'] = df['rgs_dt'].apply(replace_with_nearest_future_weekday)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3ea37",
   "metadata": {},
   "source": [
    "## 3-1) Finance Data를 불러오지 못하는 기업 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9740025",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = len(df)\n",
    "count=0\n",
    "counter=0\n",
    "Dlist=[]\n",
    "Error_tick=[]\n",
    "for i in range (0, stop):\n",
    "    try:\n",
    "        if len(ReadingData(df, i, -365, 0))>=10:\n",
    "            pass\n",
    "        else :\n",
    "            condition=df[\"tck_iem_cd\"]==df[\"tck_iem_cd\"][i]\n",
    "            df=df[~condition]\n",
    "            df=df.reset_index().drop(columns='index')\n",
    "            Dlist.append(i)\n",
    "            counter+=1\n",
    "    except Exception as e:\n",
    "        condition=df[\"tck_iem_cd\"]==df[\"tck_iem_cd\"][i]\n",
    "        df=df[~condition]\n",
    "        df=df.reset_index().drop(columns='index')\n",
    "        Dlist.append(i)\n",
    "        Error_tick.append(df[\"tck_iem_cd\"][i])\n",
    "        counter+=1\n",
    "    count+=1\n",
    "    \n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    \n",
    "df=df[:stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"train_1.csv\", index=False)\n",
    "df=pd.read_csv(\"train_1.csv\").reset_index(drop=True)\n",
    "df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff6778",
   "metadata": {},
   "source": [
    "## 3-2) summary가 되어있지 않은 데이터 제거 및 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list=[]\n",
    "\n",
    "# summary의 글자 수가 1000개 이상인 뉴스 제거\n",
    "for i in range (0, len(df[\"news_smy_ifo\"])):\n",
    "    if len(df[\"news_smy_ifo\"][i]) >= 1000:\n",
    "        drop_list.append(i)\n",
    "           \n",
    "df=df.drop(drop_list).reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "sentence_list=df[\"news_smy_ifo\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3371ec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentences = sentence_list\n",
    "results = nlp(sentences)\n",
    "sent=pd.concat([df,pd.DataFrame(results)], axis=1)\n",
    "df=sent.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a940a",
   "metadata": {},
   "source": [
    "## 3-3) summary의 불용어 제거 이후 clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 목록 다운로드\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # 영어 불용어 목록을 사용하려면 'english'를 사용합니다.\n",
    "\n",
    "corpus = df[\"news_smy_ifo\"].tolist()\n",
    "corpus_without_stopwords = []\n",
    "for sentence in corpus:\n",
    "    # 문장을 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 문장들의 문장부호 제거\n",
    "    sentence = re.sub(r'[()\\[\\]{}!,@^_&#|]', '', sentence)\n",
    "    \n",
    "    # 문장을 단어로 토큰화\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # 불용어를 제거하고 새로운 문장을 만듭니다.\n",
    "    filtered_sentence = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # 다시 문장으로 변환\n",
    "    filtered_sentence = ' '.join(filtered_sentence)\n",
    "    \n",
    "    corpus_without_stopwords.append(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "Key=[]\n",
    "\n",
    "# 데이터 프레임에서 세부사업명 열을 추출하여 리스트로 변환\n",
    "corpus2=corpus_without_stopwords.copy()\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus2)\n",
    "\n",
    "# K-means 클러스터링\n",
    "k = 29  # 클러스터 개수\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 클러스터링 결과를 데이터 프레임에 추가\n",
    "df['cluster'] = kmeans.labels_\n",
    "\n",
    "# 각 클러스터의 키워드 추출\n",
    "top_keywords = []\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(k):\n",
    "    cluster_keywords = [terms[ind] for ind in order_centroids[i, :3]]  # 상위 5개 키워드 추출\n",
    "    top_keywords.append(cluster_keywords)\n",
    "\n",
    "# 클러스터링 결과와 키워드 출력\n",
    "for cluster_id in range(k):\n",
    "    cluster_samples = df[df['cluster'] == cluster_id]\n",
    "    keywords = \", \".join(top_keywords[cluster_id])\n",
    "    Key.append(keywords)\n",
    "    # print(f\"Cluster {cluster_id}:\")\n",
    "    # print(\"Keywords:\", keywords)\n",
    "    # print(\"Samples:\")\n",
    "    # print(cluster_samples['cluster'])\n",
    "    # print('————————————')\n",
    "    \n",
    "keydf=pd.DataFrame(np.array(Key), columns=[\"keywords\"])\n",
    "keydf.index.name='cluster'\n",
    "keydf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47546771",
   "metadata": {},
   "source": [
    "## 3-4) alpha와 beta 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dafaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"15146.csv\")\n",
    "# df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y-%m-%d')\n",
    "df=df[df['tck_iem_cd']!='Corporate']\n",
    "df=df[df['tck_iem_cd']!='Fintech']\n",
    "df=df[df['tck_iem_cd']!='INAQ']\n",
    "df=df[df['tck_iem_cd']!='ATEN']\n",
    "df=df[df['tck_iem_cd']!='GHRS']\n",
    "df=df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b76dc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing libraries and packages\n",
    "import statsmodels.api as sm\n",
    "from statsmodels import regression\n",
    "\n",
    "list_alpha=[]\n",
    "list_beta=[]\n",
    "error=[]\n",
    "error_tick=[]\n",
    "count=0\n",
    "\n",
    "def linreg(x,y):\n",
    "    x = sm.add_constant(x)\n",
    "    model = regression.linear_model.OLS(y,x).fit()\n",
    "    x = x[:, 1]\n",
    "    return model.params[0], model.params[1]\n",
    "\n",
    "asset_data1=ReadingData(df, 0, -365, 0).rename(columns={\"Close\":df[\"tck_iem_cd\"][0]})\n",
    "asset_data2=ReadingData(df, len(df)-1, -365, 0).rename(columns={\"Close\":df[\"tck_iem_cd\"][len(df)-1]})\n",
    "nasdaq=pd.DataFrame(fdr.DataReader('^IXIC', start=asset_data1.index[0], end=asset_data2.index[-1]).Close).rename(columns={'Close':'NASDAQ'})\n",
    "\n",
    "for k in range(0, len(df)):\n",
    "    try:\n",
    "        asset_data = ReadingData(df, k, -365, 0).rename(columns={\"Close\": df[\"tck_iem_cd\"][k]})\n",
    "        merge = pd.concat([asset_data, nasdaq], axis=1).dropna().pct_change().dropna()\n",
    "\n",
    "        # Regression model\n",
    "        X = pd.DataFrame(merge[\"NASDAQ\"]).values\n",
    "        Y = merge.drop(columns=[\"NASDAQ\"]).values\n",
    "\n",
    "        alpha, beta = linreg(X, Y)\n",
    "\n",
    "        list_alpha.append(alpha)\n",
    "        list_beta.append(beta)\n",
    "    except Exception as e:\n",
    "        error.append(k)\n",
    "        error_tick.append(df[\"tck_iem_cd\"][k])\n",
    "        print(f\"Error for K={k}: {str(e)}\")\n",
    "        continue  # 오류가 발생해도 계속 진행\n",
    "    count+=1\n",
    "    \n",
    "    if count % 1000 == 0:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb31c62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alphabeta=pd.DataFrame([list_alpha, list_beta]).T.rename(columns={0:'alpha', 1:'beta'})\n",
    "df=pd.concat([df.drop(error).reset_index(drop=True), alphabeta], axis=1).dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83957a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"28515_ab.csv\", index=False)\n",
    "df.to_csv(\"train_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd52c15",
   "metadata": {},
   "source": [
    "## 3-5) Industry 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c736625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"28515_ab.csv\").reset_index(drop=True)\n",
    "df=pd.read_csv(\"train_2.csv\").reset_index(drop=True)\n",
    "df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fcf299",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry = pd.read_csv(\"tck_industry.csv\", index_col=0)\n",
    "ind=industry.copy()\n",
    "ind[\"industry\"]=ind[\"IndustryCode\"].apply(lambda x:x//1000000)\n",
    "ind=ind.drop_duplicates(subset='Symbol', keep='first')\n",
    "ind=ind[['Symbol','industry']].rename(columns={\"Symbol\":\"tck_iem_cd\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df, ind, on='tck_iem_cd', how='left').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"TrainingData.csv\", index=False)\n",
    "df.to_csv(\"train_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecdd1b5",
   "metadata": {},
   "source": [
    "## 3-6) Target 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93087ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"TrainingData.csv\").reset_index(drop=True)\n",
    "df=pd.read_csv(\"train_3.csv\").reset_index(drop=True)\n",
    "df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=datetime.datetime.now()\n",
    "\n",
    "MA_list=[]\n",
    "counter=0\n",
    "R=len(df)\n",
    "# R=100\n",
    "for n in range (0, R):\n",
    "    data=ReadingData(df,n,-15,15)\n",
    "    Data=MA(MA(MA(data,2),3),4).loc[:time_delta_days(df['rgs_dt'][n],4)]\n",
    "    value=Data.iloc[-5:len(Data):4].pct_change().dropna().reset_index(drop=True)\n",
    "    MA_list.append(value)\n",
    "    counter+=1\n",
    "    \n",
    "    if counter%1000==0:\n",
    "        end=datetime.datetime.now()\n",
    "        print(counter, end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e61eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거\n",
    "df=df[df['tck_iem_cd']!='CETUU'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "Concat=pd.concat(MA_list, axis=0).reset_index(drop=True)\n",
    "Concat.index=np.arange(0, len(df))\n",
    "Final=pd.concat([df,Concat], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150c509",
   "metadata": {},
   "source": [
    "## 3-7) 4월 7일에 대한 데이터 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6041243",
   "metadata": {},
   "source": [
    "**4월 7일은 Good Friday로 미국 증권시장이 닫힌 날**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d67fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=Final\n",
    "df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['rgs_dt'] == '2023-04-07', 'rgs_dt'] = '2023-04-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=datetime.datetime.now()\n",
    "\n",
    "MA_list=[]\n",
    "counter=0\n",
    "R=len(df)\n",
    "\n",
    "for n in range (7341, 7396):\n",
    "    data=ReadingData(df,n,-15,15)\n",
    "    Data=MA(MA(MA(data,2),3),4).loc[:time_delta_days(df['rgs_dt'][n],4)]\n",
    "    value=Data.iloc[-5:len(Data):4].pct_change().dropna().reset_index(drop=True)\n",
    "    MA_list.append(value)\n",
    "    counter+=1\n",
    "    \n",
    "    if counter%1000==0:\n",
    "        end=datetime.datetime.now()\n",
    "        print(counter, end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Concat47=pd.concat(MA_list, axis=0).reset_index(drop=True)\n",
    "Concat47.index=range (7341, 7396)\n",
    "df47=df.iloc[7341:7396].drop(columns=['Close', '2 MA', '3 MA', '4 MA'])\n",
    "drop47=df.drop(df.index[7341:7396])\n",
    "Final47=pd.concat([df47,Concat47], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([drop47, Final47], axis=0).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4a043",
   "metadata": {},
   "source": [
    "## 3-8) 금리 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9227bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date='2023-01-01'\n",
    "end_date='2023-12-01'\n",
    "rf=pdr.get_data_fred('FEDFUNDS', start_date, end_date)\n",
    "stock_df=fdr.DataReader(\"AAPL\", start=start_date, end=end_date)[['Close']]\n",
    "fed_funds_df=rf.copy()\n",
    "fed_funds_df=fed_funds_df.reset_index()\n",
    "stock_df=stock_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 열을 Datetime 형식으로 변환\n",
    "fed_funds_df['DATE'] = pd.to_datetime(fed_funds_df['DATE'])\n",
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "\n",
    "# 월(Month) 열을 추출\n",
    "fed_funds_df['Month'] = fed_funds_df['DATE'].dt.to_period('M')\n",
    "stock_df['Month'] = stock_df['Date'].dt.to_period('M')\n",
    "\n",
    "# 월별로 그룹화하여 월별 금리 데이터를 추가\n",
    "merged_df = stock_df.copy()  # 결과를 저장할 DataFrame 복제\n",
    "\n",
    "for month, group in stock_df.groupby('Month'):\n",
    "    month_fed_funds = fed_funds_df[fed_funds_df['Month'] == month]\n",
    "    if not month_fed_funds.empty:\n",
    "        merged_df.loc[group.index, 'FEDFUNDS'] = month_fed_funds['FEDFUNDS'].values[0]\n",
    "\n",
    "# 결과 출력\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695607de",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=merged_df.rename(columns={\"Date\":\"rgs_dt\", \"FEDFUNDS\":\"R_f\"})\n",
    "merged_df=merged_df[['rgs_dt','R_f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d53076",
   "metadata": {},
   "outputs": [],
   "source": [
    "data8=pd.merge(df, merged_df, how='inner', on='rgs_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3d4c8",
   "metadata": {},
   "source": [
    "## 3-9) 환율 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfba946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data8\n",
    "df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KRW=fdr.DataReader('KRW/USD',start=start_date)[['Close']].rename(columns={\"Close\":\"KRW/USD\"})\n",
    "CNY=fdr.DataReader('CNY/USD',start=start_date)[['Close']].rename(columns={\"Close\":\"CNY/USD\"})\n",
    "JPY=fdr.DataReader('JPY/USD',start=start_date)[['Close']].rename(columns={\"Close\":\"JPY/USD\"})\n",
    "EUR=fdr.DataReader('EUR/USD',start=start_date)[['Close']].rename(columns={\"Close\":\"EUR/USD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc78281",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_rate=pd.concat([KRW,CNY,JPY,EUR],axis=1)\n",
    "ex_rate=ex_rate.reset_index().rename(columns={\"Date\":\"rgs_dt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd53fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data9=pd.merge(df, ex_rate, how='inner', on='rgs_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20d220",
   "metadata": {},
   "source": [
    "## 3-9. MA 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf27d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label : 긍/부정+중립\n",
    "df=data9\n",
    "df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y-%m-%d')\n",
    "df=df.drop(['Close', '2 MA', '3 MA', '4 MA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb27529",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=datetime.datetime.now()\n",
    "\n",
    "ma_df_list=[]\n",
    "counter=0\n",
    "\n",
    "error=[]\n",
    "error_tick=[]\n",
    "\n",
    "for n in range (0, len(df)):\n",
    "    try:\n",
    "        data=ReadingData(df,n,-50,60)\n",
    "        MovAvg=MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(MA(data,2),3),4),5),6),7),8),9),10),11),12),13),14),15),16),17),18),19),20),21),22),23),24),25),26),27),28),29),30).dropna()\n",
    "        # MovAvg.loc[:time_delta_days(df['rgs_dt'][n],0)]\n",
    "        # MovAvg.loc[time_delta_days(df['rgs_dt'][n],0):time_delta_days(df['rgs_dt'][n],30)]\n",
    "        MAvg=MovAvg.loc[time_delta_days(df['rgs_dt'][n],0):]\n",
    "        befMAvg=MovAvg.loc[:time_delta_days(df['rgs_dt'][n],0)].iloc[-2:-1:][0:1]\n",
    "        befMAvg\n",
    "\n",
    "        MA1_after=MAvg.loc[time_delta_days(df['rgs_dt'][n],1-1):]['Close'].reset_index(drop=True)\n",
    "        for m in range (2, 8):\n",
    "            exec(f\"MA{m}_after = MAvg.loc[time_delta_days(df['rgs_dt'][n],{m}-1):]['{m} MA'].reset_index(drop=True)\")\n",
    "\n",
    "        MovingA=pd.concat([MA1_after, MA2_after, MA3_after, MA4_after, MA5_after, MA6_after ,MA7_after], axis=1).dropna()\n",
    "        MovingA=MovingA[:30]\n",
    "\n",
    "        MA_lists = [[] for _ in range(7)]\n",
    "        MA_lists[0]=((MovingA['Close']-befMAvg['Close'][0])/befMAvg['Close'][0])\n",
    "        for i in range (1, 7):\n",
    "            MA_lists[i]=((MovingA[f'{i+1} MA']-befMAvg[f'{i+1} MA'][0])/befMAvg[f'{i+1} MA'][0])\n",
    "        MA_df=pd.DataFrame(MA_lists).T.iloc[:30].rename(columns={'Close':'1 MA'})\n",
    "\n",
    "        LIST=[]\n",
    "        for M in range (1, 8):\n",
    "            for iloc in range(0, len(MA_df)):\n",
    "                LIST.append(MA_df.iloc[iloc][f'{M} MA'])\n",
    "        LIST2=[]\n",
    "        ordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n//10%10!=1)*(n%10<4)*n%10::4])\n",
    "        for M in range (1, 8):\n",
    "            for iloc in range(0, len(MA_df)):\n",
    "                LIST2.append(f'{M} MA '+ordinal(iloc+1))\n",
    "        ma_df=pd.DataFrame(LIST).T\n",
    "        ma_df.columns=LIST2\n",
    "        \n",
    "        ma_df_list.append(ma_df)\n",
    "    except Exception as e:\n",
    "        error.append(n)\n",
    "        error_tick.append(df[\"tck_iem_cd\"][n])\n",
    "        print(f\"Error for K={n}: {str(e)}\")\n",
    "        continue  # 오류가 발생해도 계속 진행\n",
    "    counter+=1\n",
    "    \n",
    "    if counter%1000==0:\n",
    "        end=datetime.datetime.now()\n",
    "        print(counter, end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#최종 Train Data\n",
    "df=df.drop(error).reset_index(drop=True)\n",
    "MA_DF=pd.concat(ma_df_list).reset_index(drop=True)\n",
    "final_data=pd.concat([df,MA_DF], axis=1)\n",
    "final_data.to_csv(\"Final_Train_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb89d6",
   "metadata": {},
   "source": [
    "# 4. 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32630b76",
   "metadata": {},
   "source": [
    "## 4-1. 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Final_Train_Data.csv')\n",
    "df=data.copy()\n",
    "\n",
    "#불필요 열 삭제\n",
    "df=df.drop(['rgs_dt', 'tck_iem_cd', 'til_ifo', 'ctgy_cfc_ifo', 'mdi_ifo', 'news_smy_ifo', 'rld_ose_iem_tck_cd', 'url_ifo', 'KRW/USD', 'JPY/USD'], axis=1)\n",
    "\n",
    "#컬럼명 변경\n",
    "df=df.rename(columns={'label': 'sent', \n",
    "                   'score': 'conf', \n",
    "                   'cluster': 'event',\n",
    "                    'R_f': 'ir'})\n",
    "\n",
    "event_column = df.pop('event')\n",
    "df['event'] = event_column\n",
    "\n",
    "#컬럼 순서 변경\n",
    "cols = list(df.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "df = df[cols]\n",
    "\n",
    "#컬럼 분리\n",
    "cn = df.columns.tolist()\n",
    "feature, MA1, MA2, MA3, MA4, MA5, MA6, MA7=cn[0:9], cn[9:39], cn[39:69], cn[69:99], cn[99:129], cn[129:159], cn[159:189], cn[189:219]\n",
    "\n",
    "#결측치 삭제\n",
    "df=df.drop(2172)\n",
    "\n",
    "#데이터타입 변경\n",
    "df.industry=df.industry.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb38e20",
   "metadata": {},
   "source": [
    "## 4-2. 그리드서치를 통한 하이퍼파라미터 최적화 및 성능테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature, target 지정\n",
    "features=df[feature]\n",
    "features = features.iloc[:, [0, 1, 5, 2, 3, 4, 6, 7, 8]]\n",
    "target = df[MA1[0]]\n",
    "\n",
    "#인코딩\n",
    "categorical = ['industry','event','sent']\n",
    "for category in categorical:\n",
    "    features[category] = features[category].astype('category')\n",
    "    features = pd.get_dummies(features, columns=[category], prefix=category, dtype='int')\n",
    "\n",
    "#train/test 데이터 분리 8:2로\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e840c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#그리드서치\n",
    "\n",
    "# 각 파라미터 값 넣기\n",
    "xgboost_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "xgboost = xgb.XGBRegressor(n_jobs=-1)\n",
    "\n",
    "grid_cv = GridSearchCV(estimator=xgboost, param_grid=xgboost_params, cv=3, n_jobs=-1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f'최적 하이퍼 파라미터: {grid_cv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 파라미터 설정\n",
    "best_xgboost_params = grid_cv.best_params_\n",
    "\n",
    "xgboost = xgb.XGBRegressor(\n",
    "    n_estimators=best_xgboost_params['n_estimators'], \n",
    "    max_depth=best_xgboost_params['max_depth'], \n",
    "    learning_rate=best_xgboost_params['learning_rate'],\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#학습\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "xgboost_pred = xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63441a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "#성능 테스트 함수\n",
    "def evaluate_xgboost_regressor(model, X, y):\n",
    "    # 모델로 예측\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # RMSE 계산\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    # MAE 계산\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    \n",
    "    # R-squared 계산\n",
    "    r_squared = r2_score(y, y_pred)\n",
    "    \n",
    "    # 예측값과 실제값의 오차값 리스트 생성\n",
    "    error_list = y - y_pred\n",
    "    \n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"R-squared:\", r_squared)\n",
    "    \n",
    "# 함수 사용 예시\n",
    "evaluate_xgboost_regressor(xgboost, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ed65c",
   "metadata": {},
   "source": [
    "## 4-3. 모델링 및 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb899aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAs=[MA1, MA2, MA3, MA4, MA5, MA6, MA7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df[feature]\n",
    "X_train = X_train.iloc[:, [0, 1, 5, 2, 3, 4, 6, 7, 8]]\n",
    "\n",
    "events=pd.Series(X_train.iloc[:,0].unique()).sort_values().tolist()\n",
    "sents=pd.Series(X_train.iloc[:,1].unique()).sort_values().tolist()\n",
    "industries=pd.Series(X_train.iloc[:,2].unique()).sort_values().tolist()\n",
    "\n",
    "events, sents, industries = [str(x) for x in events], [str(x) for x in sents], [str(x) for x in industries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb4917",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.DataFrame(columns=X_train.columns)\n",
    "\n",
    "for event in events:\n",
    "    for sent in sents:\n",
    "        for industry in industries:\n",
    "            new_row=[event, sent, industry, df.conf.mean(), df.alpha.mean(), df.beta.mean(), df.ir.mean(), df['CNY/USD'].mean(), df['EUR/USD'].mean()]\n",
    "            pred_data = pred_data.append(pd.Series(new_row, index=X_train.columns), ignore_index=True)\n",
    "            \n",
    "result=pred_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d480459",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "\n",
    "for MA in MAs:\n",
    "    for n in MA:\n",
    "        \n",
    "        if count==0:\n",
    "            #X_train 생성 및 인코딩\n",
    "            X_train=df[feature]\n",
    "            X_train = X_train.iloc[:, [0, 1, 5, 2, 3, 4, 6, 7, 8]]\n",
    "\n",
    "            categorical = ['industry','event','sent']\n",
    "            for category in categorical:\n",
    "                X_train[category] = X_train[category].astype('category')\n",
    "                X_train = pd.get_dummies(X_train, columns=[category], prefix=category, dtype='int') \n",
    "\n",
    "            # 최적 파라미터 설정\n",
    "            best_xgboost_params = grid_cv.best_params_\n",
    "\n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=best_xgboost_params['n_estimators'], \n",
    "                max_depth=best_xgboost_params['max_depth'], \n",
    "                learning_rate=best_xgboost_params['learning_rate'],\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        y_train = df[n]\n",
    "\n",
    "        #학습\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        if count==0:\n",
    "            #결과용 데이터 인코딩 및 컬럼 정리\n",
    "            categorical = ['industry','event','sent']\n",
    "            for category in categorical:\n",
    "                pred_data[category] = pred_data[category].astype('category')\n",
    "                pred_data = pd.get_dummies(pred_data, columns=[category], prefix=category, dtype='int')\n",
    "\n",
    "            pred_data = pred_data[X_train.columns]\n",
    "            \n",
    "            count+=1\n",
    "    \n",
    "        model_pred = model.predict(pred_data)\n",
    "        model_pred = list(model_pred)\n",
    "        result[n] = model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#결과 저장\n",
    "result.to_csv('final_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95499c9",
   "metadata": {},
   "source": [
    "## 4-4. 결과 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46667b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation=result.copy()\n",
    "\n",
    "#industry re-labeling\n",
    "interpretation['industry']=interpretation['industry'].astype(int)\n",
    "interpretation['industry'].replace({0: 'Services', \n",
    "                       1: 'Others',\n",
    "                       50: 'fossil fuel', \n",
    "                       51: 'chemical/mineral', \n",
    "                       52: 'heavy', \n",
    "                       53: 'accommodation/distribution', \n",
    "                       54: 'consumer/food',\n",
    "                       55: 'finance', \n",
    "                       56: 'medical/pharmaceutical', \n",
    "                       57: 'semiconductor/IT', \n",
    "                       59: 'eco-friendly energy', \n",
    "                       60: 'real estate', \n",
    "                       62: 'legal',\n",
    "                       63: 'edu'\n",
    "                       }, inplace=True)\n",
    "\n",
    "#event re-labeling\n",
    "interpretation['event']=interpretation['event'].astype(int)\n",
    "'''\n",
    "interpretation['event'].replace({0: 'guru stock report', \n",
    "                       1: 'Fintel share report',\n",
    "                       2: 'Fintel coverage recommendation', \n",
    "                       3: 'quarterly loss', \n",
    "                       4: 'recent fluctuations in shares',\n",
    "                       5: 'nasdaq trading', \n",
    "                       6: 'performance and option', \n",
    "                       7: 'quarterly share and Zacks', \n",
    "                       8: 'share price fluctuations compared to moving average', \n",
    "                       9: 'stock fluctuations', \n",
    "                       10: 'quarterly revenue report', \n",
    "                       11: 'performance reccomandation', \n",
    "                       12: 'Tech stocks', \n",
    "                       13: 'Investors',  \n",
    "                       14: 'dividend channel',  \n",
    "                       15: 'declaring dividends by board',  \n",
    "                       16: 'recommandation coverage report',  \n",
    "                       17: 'investors and stock report',  \n",
    "                       18: 'Chicago related news',  \n",
    "                       19: 'stock recommendation',  \n",
    "                       20: 'consumer and financial stocks',  \n",
    "                       21: 'guru financial report',  \n",
    "                       22: 'option trading',  \n",
    "                       23: 'health care stock',  \n",
    "                       24: 'NASDAQ100, RTT news info',  \n",
    "                       25: 'Fintel reports',  \n",
    "                       26: 'buy recommandation',  \n",
    "                       27: 'year-to-date stock report',  \n",
    "                       28: 'Others',  \n",
    "                       }, inplace=True)\n",
    "'''\n",
    "interpretation['event'].replace({4: 'recent fluctuations in shares',  }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3fc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance 도출\n",
    "\n",
    "ftr=interpretation.iloc[:, :9]\n",
    "for category in categorical:\n",
    "    ftr[category] = ftr[category].astype('category')\n",
    "    ftr = pd.get_dummies(ftr, columns=[category], prefix=category, dtype='int') \n",
    "\n",
    "FI = pd.DataFrame({'Feature': ftr.columns})\n",
    "FI['Importance'] = pd.Series(model.feature_importances_)\n",
    "FI=FI.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#예시로 MA=3으로 설정한 데이터 가져오기\n",
    "\n",
    "df_MA3=interpretation.copy()\n",
    "df_MA3=df_MA3[['event', 'sent', 'industry'] + MA3]\n",
    "\n",
    "#열 이름 변경하기\n",
    "import re\n",
    "pattern = r'3 MA (\\d+)'\n",
    "\n",
    "# 열 이름 변경 함수\n",
    "def rename_columns(col_name):\n",
    "    match = re.match(pattern, col_name)\n",
    "    if match:\n",
    "        return match.group(1)  # 정규식 패턴에 맞는 숫자 부분을 반환\n",
    "    else:\n",
    "        return col_name\n",
    "\n",
    "# 열 이름 변경 적용\n",
    "df_MA3.columns = df_MA3.columns.map(rename_columns)\n",
    "\n",
    "df_MA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#MA 수익률 시각화 함수\n",
    "#input: 산업, 이벤트\n",
    "def MAER_Visualization(df, industry, event): \n",
    "    \n",
    "    df=df[df['industry']==industry] #산업 선택\n",
    "    df=df[df['event']==event]     #이벤트 선택\n",
    "\n",
    "    data_to_plot_1 = df.iloc[0, 3:] #부정적 이벤트\n",
    "    data_to_plot_2 = df.iloc[1, 3:] #중립적 이벤트\n",
    "    data_to_plot_3 = df.iloc[2, 3:] #긍정적 이벤트\n",
    "\n",
    "    plt.plot(data_to_plot_1, color='red', label='Negative')\n",
    "    plt.plot(data_to_plot_2, color='black', label='Neutral')\n",
    "    plt.plot(data_to_plot_3, color='blue', label='Positive')\n",
    "\n",
    "    title='Changes in MAER when articles about '+event+' are published regarding the '+industry+' industry'\n",
    "    industry+' 산업에 '+event+' 관련 기사가 났을 때의 이동평균 변화 추이'\n",
    "\n",
    "    # 그래프 제목과 축 레이블 설정 (옵션)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('MA Earning Rate')\n",
    "\n",
    "    # 범례 추가\n",
    "    plt.legend()\n",
    "\n",
    "    # 그래프 표시\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#예시\n",
    "MAER_Visualization(df_MA3, 'semiconductor/IT', 'recent fluctuations in shares')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab09a22",
   "metadata": {},
   "source": [
    "# 5. 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ceb5a",
   "metadata": {},
   "source": [
    "## 5-1. Elbow Method (text clustering: k 결정하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e7ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import FinanceDataReader as fdr\n",
    "import pandas_datareader as pdr\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import plotly.express as px\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pytimekr import pytimekr\n",
    "import datetime\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "import warnings                 # warning 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ssl    # Mac에서 FinanceDataReader가 안나오는 오류 해결\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from matplotlib import rc       # 한글 깨짐 방지\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "#데이터 불러오기\n",
    "df=pd.read_csv(\"Final_TrainData.csv\").reset_index(drop=True)\n",
    "df['rgs_dt'] = pd.to_datetime(df['rgs_dt'], format='%Y-%m-%d')\n",
    "df=df.drop(columns=['cluster'])\n",
    "\n",
    "#함수 정의\n",
    "from sklearn.cluster import KMeans\n",
    "def visualize_elbowmethod(data, N, param_init='random', param_n_init=10, param_max_iter=300):\n",
    "    distortions = []\n",
    "    for i in range(1, N):\n",
    "        km = KMeans(n_clusters=i, init=param_init, n_init=param_n_init, max_iter=param_max_iter, random_state=0)\n",
    "        km.fit(data)\n",
    "        distortions.append(km.inertia_)\n",
    "\n",
    "    plt.plot(range(1, N), distortions, marker='o')\n",
    "    plt.xlabel('Number of Cluster')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.show()\n",
    "    \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "def visualize_silhouette_layer(data, N, param_init='random', param_n_init=10, param_max_iter=300):\n",
    "    clusters_range = range(2,N+1)\n",
    "    results = []\n",
    "\n",
    "    for i in clusters_range:\n",
    "        clusterer = KMeans(n_clusters=i, init=param_init, n_init=param_n_init, max_iter=param_max_iter, random_state=0)\n",
    "        cluster_labels = clusterer.fit_predict(data)\n",
    "        silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "        results.append([i, silhouette_avg])\n",
    "\n",
    "    result = pd.DataFrame(results, columns=[\"n_clusters\", \"silhouette_score\"])\n",
    "    pivot_km = pd.pivot_table(result, index=\"n_clusters\", values=\"silhouette_score\")\n",
    "\n",
    "    plt.figure()\n",
    "    sns.heatmap(pivot_km, annot=True, linewidths=.5, fmt='.3f', cmap=sns.cm._rocket_lut)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# 불용어 목록 다운로드\n",
    "stop_words = set(stopwords.words('english'))  # 영어 불용어 목록을 사용하려면 'english'를 사용합니다.\n",
    "\n",
    "corpus = df[\"news_smy_ifo\"].tolist()\n",
    "corpus_without_stopwords = []\n",
    "for sentence in corpus:\n",
    "    # 문장을 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 문장들의 문장부호 제거\n",
    "    sentence = re.sub(r'[()\\[\\]{}!,@^_&#|]', '', sentence)\n",
    "    \n",
    "    # 문장을 단어로 토큰화\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # 불용어를 제거하고 새로운 문장을 만듭니다.\n",
    "    filtered_sentence = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # 다시 문장으로 변환\n",
    "    filtered_sentence = ' '.join(filtered_sentence)\n",
    "    \n",
    "    corpus_without_stopwords.append(filtered_sentence)\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "Key=[]\n",
    "\n",
    "# 데이터 프레임에서 세부사업명 열을 추출하여 리스트로 변환\n",
    "corpus2=corpus_without_stopwords.copy()\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus2)\n",
    "\n",
    "#시각화\n",
    "visualize_elbowmethod(X, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aedaa6",
   "metadata": {},
   "source": [
    "# 5-2. Industry 산업 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=pd.read_csv(\"tck_industry.csv\")\n",
    "\n",
    "indCode=df['industry'].unique().tolist()\n",
    "\n",
    "for code in indCode:\n",
    "    A=ind0[ind0['IndustryCode']//1000000==code]\n",
    "    print(code,\":\")\n",
    "    print(A['Industry'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843b967",
   "metadata": {},
   "source": [
    "# 6. 강화학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5639af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(industry):\n",
    "    episode_list = []\n",
    "    episode_name = []\n",
    "    path = os.getcwd()\n",
    "    tck_path = '{}/Data/{}'.format(path,industry)\n",
    "    \n",
    "    tck_list = os.listdir(tck_path)\n",
    "    for tck in tck_list:\n",
    "        \n",
    "        event_path = f'{path}/Data/{industry}/{tck}'\n",
    "        event_list = os.listdir(event_path)\n",
    "        \n",
    "        for event in event_list:\n",
    "            os.chdir(event_path)\n",
    "            df = pd.read_csv(event, dtype={'code': str}, parse_dates=['Date'])\n",
    "            df['action'] = 0\n",
    "            df = df.drop(columns=['Date'])\n",
    "\n",
    "            episode_list.append(df)\n",
    "            \n",
    "    return episode_list\n",
    "\n",
    "# 각 이벤트를 리스트에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_chart(data, buy_signals, sell_signals):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data['Adj Close'], label='Price', alpha=0.7)\n",
    "\n",
    "    # Correcting the scatter function\n",
    "    if buy_signals:\n",
    "        buy_x, buy_y = zip(*buy_signals)\n",
    "        plt.scatter(buy_x, buy_y, color='g', marker='^', label='Buy Signal')\n",
    "\n",
    "    if sell_signals:\n",
    "        sell_x, sell_y = zip(*sell_signals)\n",
    "        plt.scatter(sell_x, sell_y, color='r', marker='v', label='Sell Signal')\n",
    "\n",
    "    plt.title('Price Chart with Buy/Sell Signals')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_portfolio_values(portfolio_values):\n",
    "    plt.plot(portfolio_values)\n",
    "    plt.title('Portfolio Value Over Time')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def I_MR(ma_ratio):\n",
    "    if ma_ratio > 1:\n",
    "        return -1\n",
    "    elif ma_ratio <= 1:\n",
    "        return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd74969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment:\n",
    "    def __init__(self, episode_data):\n",
    "        self.episode_data = episode_data\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(episode_data)\n",
    "        self.transaction_cost = 0.01  # Adjust this based on your scenario\n",
    "        self.initial_balance = 1000000  # Adjust this based on your scenario\n",
    "        self.balance = self.initial_balance\n",
    "        self.num_stocks = 0\n",
    "        self.portfolio_value = self.balance\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.num_stocks = 0\n",
    "        self.portfolio_value = self.balance\n",
    "\n",
    "\n",
    "    def get_state(self):\n",
    "        state = self.episode_data.iloc[self.current_step].to_dict()\n",
    "\n",
    "        # Convert non-numerical values to numbers if possible\n",
    "        for key, value in state.items():\n",
    "            if key != 'Date':\n",
    "                try:\n",
    "                    state[key] = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    # Use a default value or skip the key if conversion is not possible\n",
    "                    state[key] = 0.0  # You can change this default value as needed\n",
    "\n",
    "        return state\n",
    "\n",
    "    def decide_buying_unit(self,ma_ratio, max_buyable_stocks):\n",
    "        if ma_ratio <=0.6:\n",
    "            return max(1, int(max_buyable_stocks))\n",
    "        elif ma_ratio <=0.8:\n",
    "            return max(1, int(0.75*int(max_buyable_stocks)))\n",
    "        else:\n",
    "            return max(1, int(0.2*int(max_buyable_stocks)))\n",
    "        \n",
    "    def decide_selling_unit(self,ma_ratio, max_sellable_stocks):\n",
    "        if ma_ratio >=1.4:\n",
    "            return max(1, int(max_sellable_stocks))\n",
    "        elif ma_ratio <=1.2:\n",
    "            return max(1, int(0.75*int(max_sellable_stocks)))\n",
    "        else:\n",
    "            return max(1, int(0.2*int(max_sellable_stocks)))\n",
    "    \n",
    "    \n",
    "    def take_action(self, action):\n",
    "        current_price = self.episode_data.iloc[self.current_step]['Adj Close']\n",
    "\n",
    "        # Previous action\n",
    "        prev_action = 0 if self.current_step == 0 else self.episode_data.iloc[self.current_step - 1]['action']\n",
    "\n",
    "        # Transaction cost\n",
    "        transaction_cost = int(prev_action != action) * self.transaction_cost \n",
    "\n",
    "        # Moving Average Ratio\n",
    "        ma_ratio = int(self.episode_data.iloc[self.current_step]['MA_ratio'])\n",
    "        # Reward calculation\n",
    "        \n",
    "        \n",
    "        if action == 0:  # Sell\n",
    "            if self.num_stocks > 0:  # Ensure there are stocks to sell\n",
    "                max_sellable_stocks = self.num_stocks\n",
    "                trading_unit = min(max_sellable_stocks, self.decide_selling_unit(ma_ratio,max_sellable_stocks))\n",
    "                sell_amount = current_price * (1 - self.transaction_cost) * trading_unit\n",
    "                self.balance += sell_amount\n",
    "                self.num_stocks -= trading_unit\n",
    "            reward = (action-1) * I_MR(ma_ratio) - transaction_cost\n",
    "            \n",
    "        elif action ==1 : # Hold\n",
    "            if ma_ratio > 1.2:\n",
    "                reward = 0.8\n",
    "            elif ma_ratio >= 0.8:\n",
    "                reward = 1.2\n",
    "            elif ma_ratio < 0.8:\n",
    "                reward = 0.8\n",
    "        elif action == 2:  # Buy\n",
    "            max_buyable_stocks = int(self.balance / (current_price * (1 + self.transaction_cost)))\n",
    "            trading_unit = min(max_buyable_stocks, self.decide_buying_unit(ma_ratio,max_buyable_stocks))\n",
    "            buy_amount = current_price * (1 + self.transaction_cost) * trading_unit\n",
    "\n",
    "            if self.balance >= buy_amount:  # Ensure there is enough balance to buy\n",
    "                self.balance -= buy_amount\n",
    "                self.num_stocks += trading_unit\n",
    "            reward = (action-1) * I_MR(ma_ratio) - transaction_cost\n",
    "\n",
    "        self.portfolio_value = self.balance + current_price * self.num_stocks\n",
    "        self.current_step += 1\n",
    "        done = self.current_step == self.max_steps - 1\n",
    "        if self.current_step == len(self.episode_data)-1:\n",
    "            done = True\n",
    "\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.7, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Q-networks\n",
    "        self.q_network = QNetwork(state_size, action_size)\n",
    "        self.target_network = QNetwork(state_size, action_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())  # Copy initial weights\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        state_values = [value for key, value in state.items()]\n",
    "        state_tensor = torch.FloatTensor(state_values).unsqueeze(0)\n",
    "\n",
    "        q_values = self.q_network(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        state_values = [value for key, value in state.items()]\n",
    "        state_tensor = torch.FloatTensor(state_values).unsqueeze(0)\n",
    "        \n",
    "        next_state_values = [value for key, value in next_state.items()]\n",
    "        next_state_tensor = torch.FloatTensor(next_state_values).unsqueeze(0)\n",
    "        \n",
    "        action = torch.LongTensor([action])\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        done = torch.FloatTensor([int(done)])\n",
    "\n",
    "        # Q-value prediction for the current state\n",
    "        q_values = self.q_network(state_tensor).gather(1, action.unsqueeze(1))\n",
    "\n",
    "        # Q-value prediction for the next state\n",
    "        next_q_values = self.target_network(next_state_tensor).detach().max(1)[0].unsqueeze(1)\n",
    "        target = reward + (1 - done) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        loss = nn.functional.smooth_l1_loss(q_values, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Update target network weights every few steps (e.g., every 10 episodes)\n",
    "        if self.current_step % 10 == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.current_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)  \n",
    "        self.fc4 = nn.Linear(hidden_size, action_size)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)  \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_list = load_data('Industry_50')\n",
    "\n",
    "# 데이터 분할\n",
    "train_episodes, test_episodes = train_test_split(episode_list, test_size=0.05, random_state=42)\n",
    "\n",
    "# Initialize the agent\n",
    "\n",
    "state_size = 13\n",
    "action_size = 3  \n",
    "agent = DQNAgent(state_size=state_size, action_size=action_size)\n",
    "\n",
    "\n",
    "train_num_episodes = len(train_episodes)\n",
    "for train_episode_data in train_episodes:\n",
    "    train_env = TradingEnvironment(train_episode_data)\n",
    "    train_env.reset()\n",
    "    total_reward = 0\n",
    "    portfolio_values = []\n",
    "\n",
    "    for t in range(29):\n",
    "        train_state = train_env.get_state()\n",
    "        train_action = agent.act(train_state)\n",
    "        train_reward, train_done = train_env.take_action(train_action)\n",
    "        train_next_state = train_env.get_state()\n",
    "        agent.train(train_state, train_action, train_reward, train_next_state, train_done)\n",
    "\n",
    "        total_reward += train_reward\n",
    "        portfolio_values.append(train_env.portfolio_value)\n",
    "#         print(f\"Step {t}: Action - {train_action}, Reward - {train_reward}, Portfolio Value - {train_env.portfolio_value}\")\n",
    "\n",
    "\n",
    "#     print(f\"Total Reward: {total_reward}\")\n",
    "#     print(f\"Final Portfolio Value: {train_env.portfolio_value}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing loop\n",
    "test_num_episodes = len(test_episodes)\n",
    "for test_episode_data in test_episodes:\n",
    "    test_env = TradingEnvironment(test_episode_data)\n",
    "    test_env.reset()\n",
    "    total_reward = 0\n",
    "    portfolio_values = []\n",
    "\n",
    "    # Lists to store buy and sell signals\n",
    "    buy_signals = []\n",
    "    sell_signals = []\n",
    "    hold_postion = []\n",
    "\n",
    "    for t in range(29):\n",
    "        test_state = test_env.get_state()\n",
    "        test_action = agent.act(test_state)\n",
    "        test_reward, test_done = test_env.take_action(test_action)\n",
    "        test_next_state = test_env.get_state()\n",
    "\n",
    "        total_reward += test_reward\n",
    "\n",
    "        portfolio_values.append(test_env.portfolio_value)\n",
    "#         print(f\"Step {t}: Action - {test_action}, Reward - {test_reward}, Portfolio Value - {test_env.portfolio_value}\")\n",
    "\n",
    "        # Record buy and sell signals\n",
    "        if test_action == 2:  # Buy (long)\n",
    "            buy_signals.append((t, test_episode_data.iloc[t]['Adj Close']))\n",
    "        elif test_action == 0:  # Sell (short)\n",
    "            sell_signals.append((t, test_episode_data.iloc[t]['Adj Close']))\n",
    "\n",
    "    # Print  the performance metrics\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    print(f\"Final Portfolio Value: {test_env.portfolio_value}\")\n",
    "    \n",
    "#     plot_price_chart(train_episode_data, buy_signals, sell_signals)\n",
    "    plot_portfolio_values(portfolio_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
